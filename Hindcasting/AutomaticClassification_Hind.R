#The main script to run the classification. The process falls into a few main steps. First, the training data is read in as a single file. The forecasted data is then read in from several files, all generated by the automated monthly process. The ecozone intersections, however, are pre-generated.

#Should the training data not be available, you can recover the data from the historical indices. Droughts can be found from location intersections with historical CDM shapefiles. Similarly with ecozone intersection.

#Once all data is loaded in, we iterate over all groups of training and classification ecozones. Within each iteration, we train a classifier with the training data, then classify all 21 ensemble members of the forecasted data individually. Essentially, we create 21 drought forecasts where each forecast differs very slightly based, originally, off the differences in raw forecast. Of course, these differences in raw forecasts translate to differences in SPI, SPEI, and PDI.

#We also export the previous (or current) drought state. This allows easy comparison between previous and future drought states, allowing us to determine if the drought increased or decreased (as opposed to just knowing the future state).

#I recommend anyone who uses this to try to familiarize themselves with it. Try taking it apart, and in particular changing neural network hyperparameters and data subsampling. This classification is not fully optimal, so changes should be made as time goes on.

library(keras)
library(parallel)
library(doParallel)
library(foreach)

Normalize = function(InVec, Max, Min){
  OutVec = (InVec - Min) / (Max - Min)
  #Won't affect the training run, but forecast values outside range will be set to the min or max 
  OutVec[which(OutVec > 1)] = 1
  OutVec[which(OutVec < 0)] = 0
  return(OutVec)
}

######### Set the date and the directory accordingly!

HindcastDate = as.Date(commandArgs(trailingOnly = TRUE)[2])
MainDirectory = paste0(commandArgs(trailingOnly = TRUE)[3], '\\')

#########


#Determine the original date of forecast - this must have been run (to acquire the previous CDM) in order to run the hindcast
HindYear = format(HindcastDate, format="%Y")
HindMonth = format(HindcastDate, format="%m")
for(Day in 1:7){
  DayString = paste0(0, toString(Day))
  ForecastDate = as.Date(paste0(HindYear, '-', HindMonth, '-', DayString))
  if(weekdays(ForecastDate) == "Thursday") break
}


IndexCount = (12*2) + 1 + 2 + 2 + 1 + 7  #Not just indices - 12 months of SPI, SPEI + 1 PDI value + Lat/Lon + Year/Month + Ecozone + current and past 6 months of drought. We only use the last month, but previous months could be included as a potential way of separating long and short term drought.


#These two groupings should be identical, but training groups could include extra ecozones. For example, we want to classify ecozone N. Ecozone N has very poor training data, so we want to train the classifier with ecozone N and M. We can later train another classifier only on ecozone M for the classification of ecozone M. This allows us to leverage the better training data of ecozone M without degrading the classification of ecozone M.
TrainGroups = list(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))
ClassifyGroups = list(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))

ClassificationOutputDir = paste0(MainDirectory, "Outcomes\\Classifications\\")
PreviousOutputDir = paste0(MainDirectory, "Outcomes\\Prior\\")

TrainingDataFile = paste0(MainDirectory, 'Historical\\CombinedTrainingCleanEco_North.csv')
TrainingData = read.csv(TrainingDataFile)

SPIForecastDir = paste0(MainDirectory, 'Indices\\SPI\\', HindcastDate, '\\')
SPEIForecastDir = paste0(MainDirectory, 'Indices\\SPEI\\', HindcastDate, '\\')
PDIForecastDir = paste0(MainDirectory, 'Indices\\PDI\\', HindcastDate, '\\')
ForecastDroughtFile = paste0(MainDirectory, 'Indices\\CDM_Previous\\', ForecastDate, '\\CDM.csv')

#SPIForecastDir0 = paste0(SPIForecastDir, '0', '\\')
#SPEIForecastDir0 = paste0(SPEIForecastDir, '0', '\\')
#SPIFormatFiles = list.files(SPIForecastDir0)
#SPEIFormatFiles = list.files(SPEIForecastDir0)

#IntFiles = intersect(SPIFormatFiles, SPEIFormatFiles)     #Every index of both forecast types will have these locations

SPIForecastDir0 = paste0(SPIForecastDir, '0', '\\')
SPEIForecastDir0 = paste0(SPEIForecastDir, '0', '\\')
SPIFormatFiles = list.files(SPIForecastDir0)
SPEIFormatFiles = list.files(SPEIForecastDir0)

IntFiles = intersect(SPIFormatFiles, SPEIFormatFiles)     #Every index of both forecast types will have these locations

IntFiles2 = read.csv(paste0(MainDirectory, '\\Misc\\ForecastingPoints.csv'))
IntFiles2 = paste0(apply(IntFiles2, 1, paste0, collapse = '_'), '.csv')
IntFiles3 = intersect(IntFiles, IntFiles2)     #Every index of both forecast types will have these locations

IntFiles = IntFiles3

PredictArray = array(0, c(length(IntFiles), IndexCount))    #The 21 is for the 21 ensemble members

for(FileNum in 1:length(IntFiles)){
  File = IntFiles[FileNum]
  
  LatLon = unlist(strsplit(File,'.csv'))[1]   #Take the file's geographical location from the file name
  Lat = unlist(strsplit(LatLon,'_'))[1]
  Lon = unlist(strsplit(LatLon,'_'))[2]
  
  Member = 0
    
  SPIForecastDirMem = paste0(SPIForecastDir, Member, '\\')
  SPEIForecastDirMem = paste0(SPEIForecastDir, Member, '\\')
  PDIForecastDirMem = paste0(PDIForecastDir, Member, '\\')
  
  SPIData = read.table(paste0(SPIForecastDirMem, File), header=FALSE)  #SPI has no header. We want the first 14 cols (Year, month, SPI 1-12)
  SPEIData = read.csv(paste0(SPEIForecastDirMem, File))    #SPEI and PDI have headers. We want columns 3-14 for SPEI 1-12 (1 and 2 are year, month but we already have that from SPI)
  PDIData = read.csv(paste0(PDIForecastDirMem, LatLon, '_PDI.csv'))    #We only want column 4 for the PDI value.
  
  SPIData = as.matrix(SPIData)[dim(SPIData)[1],1:14]          #Only take the last month, as represented by the first index being equal to the first
  SPEIData = as.matrix(SPEIData)[dim(SPEIData)[1],3:14]       #index length. The second index represents, for example, the one to twelve month SPI.
  PDIData = as.matrix(PDIData)[dim(PDIData)[1],4]
  
  PredictArray[FileNum,1] = Lat
  PredictArray[FileNum,2] = Lon
  PredictArray[FileNum,3:16] = SPIData
  PredictArray[FileNum,17:28] = SPEIData
  PredictArray[FileNum,29] = PDIData
    
  
}

#The previous block returned a list of subset predict arrays - arranged in a way that if bound on index 2, they would form a full block of predicted drought indices. The following section steps through each list item and merges them together into PredictArray, which is a singular array.


DroughtData = read.csv(ForecastDroughtFile)
DroughtData = as.matrix(DroughtData)
colnames(DroughtData) = NULL
DroughtData = as.array(DroughtData[,1:ncol(DroughtData)])

for(DroughtLoc in 1:(dim(DroughtData)[1])){
  Loc = intersect(which(PredictArray[,1] == DroughtData[DroughtLoc,2]),which(PredictArray[,2] == DroughtData[DroughtLoc,3]))
  PredictArray[Loc,31] = DroughtData[DroughtLoc, dim(DroughtData)[2]]
}


TrainMatFull = as.matrix(TrainingData)


class(TrainMatFull) = 'numeric'
colnames(TrainMatFull) = c('Lat', 'Lon', 'Year', 'Month', 'SPI1', 'SPI2', 'SPI3', 'SPI4', 'SPI5', 'SPI6', 'SPI7', 'SPI8', 'SPI9', 'SPI10', 'SPI11', 'SPI12'  , 'SPEI1', 'SPEI2', 'SPEI3', 'SPEI4', 'SPEI5', 'SPEI6', 'SPEI7', 'SPEI8', 'SPEI9','SPEI10', 'SPEI11', 'SPEI12',  'PDI', 'Drought', 'Drought-1', 'Drought-2', 'Drought-3', 'Drought-4', 'Drought-5', 'Drought-6', 'EcoZone')

TrainMatFullbk = TrainMatFull

TrainMatZone = TrainMatFull[which(TrainMatFull[,3] %in% TrainYears),]

TrainMatChange1 = TrainMatZone[which(TrainMatZone[,30] != TrainMatZone[,31]),]
TrainMatChangeNot1 = TrainMatZone[which(TrainMatZone[,30] == TrainMatZone[,31]),]

SubSampleMat = c()

Samp = sample(which(TrainMatChange1[,30] == 5), size = min(c(145, length(which(TrainMatChange1[,30] == 5)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChange1[Samp,])
Samp = sample(which(TrainMatChange1[,30] == 4), size = min(c(350, length(which(TrainMatChange1[,30] == 4)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChange1[Samp,])
Samp = sample(which(TrainMatChange1[,30] == 3), size = min(c(1700, length(which(TrainMatChange1[,30] == 3)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChange1[Samp,])
Samp = sample(which(TrainMatChange1[,30] == 2), size = min(c(3800, length(which(TrainMatChange1[,30] == 2)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChange1[Samp,])
Samp = sample(which(TrainMatChange1[,30] == 1), size = min(c(20000, length(which(TrainMatChange1[,30] == 1)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChange1[Samp,])
Samp = sample(which(TrainMatChange1[,30] == 0), size = min(c(30000, length(which(TrainMatChange1[,30] == 0)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChange1[Samp,])

Samp = sample(which(TrainMatChangeNot1[,30] == 5), size = min(c(145, length(which(TrainMatChangeNot1[,30] == 5)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChangeNot1[Samp,])
Samp = sample(which(TrainMatChangeNot1[,30] == 4), size = min(c(350, length(which(TrainMatChangeNot1[,30] == 4)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChangeNot1[Samp,])
Samp = sample(which(TrainMatChangeNot1[,30] == 3), size = min(c(1700, length(which(TrainMatChangeNot1[,30] == 3)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChangeNot1[Samp,])
Samp = sample(which(TrainMatChangeNot1[,30] == 2), size = min(c(3800, length(which(TrainMatChangeNot1[,30] == 2)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChangeNot1[Samp,])
Samp = sample(which(TrainMatChangeNot1[,30] == 1), size = min(c(20000, length(which(TrainMatChangeNot1[,30] == 1)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChangeNot1[Samp,])
Samp = sample(which(TrainMatChangeNot1[,30] == 0), size = min(c(30000, length(which(TrainMatChangeNot1[,30] == 0)))))
SubSampleMat = rbind(SubSampleMat, TrainMatChangeNot1[Samp,])


attr(TrainMatZone, 'dimnames') = NULL
attr(SubSampleMat, 'dimnames') = NULL

XTrain = as.array(SubSampleMat[,c(4:29,31)])
XTrain[which(is.infinite(XTrain))] = -4        #Assume all infs are neg infs in spi or spei caused by 0 precip months
XTrain[which(is.na(XTrain))] = -4        #Assume all infs are neg infs in spi or spei caused by 0 precip months
YTrain = as.array(SubSampleMat[,30])
class(YTrain) = 'integer'

length(which(YTrain == 4))

if(length(YTrain) == 0) next

XTrainSub = XTrain[,c(27,1,2:13,14:25,26)]
XTrainSub[which(XTrainSub[,1] == 0),1] = 1
XTrainSub[,1] = XTrainSub[,1] - 1


XTrainSubNorm = XTrainSub
NormMaxes = c()
NormMins = c()
for(i in 1:dim(XTrainSubNorm)[2]){
  NormMax = max(XTrainSubNorm[,i])
  NormMin = min(XTrainSubNorm[,i])
  
  NormMaxes = c(NormMaxes, NormMax)
  NormMins = c(NormMins, NormMin)
  
  XTrainSubNorm[,i] = Normalize(XTrainSubNorm[,i], NormMax, NormMin)
  
}

pca = prcomp(XTrainSubNorm, scale=TRUE)

YTrain[which(YTrain == 0)] = 1
YTrain = YTrain - 1

Input = pca$x[,1:8]

Random = sample(1:length(YTrain), length(YTrain), replace = FALSE)
Input = Input[Random,]
YTrain = YTrain[Random]


#########

model <- keras_model_sequential() %>%
  layer_dense(units = 60, activation = 'linear', input_shape = (dim(Input)[2])) %>%
  layer_dense(units = 20, activation = 'linear', input_shape = (dim(Input)[2])) %>%
  layer_dense(units = 6, activation = 'softmax')

# Compile model
model %>% compile(
  loss = 'sparse_categorical_crossentropy',
  optimizer = optimizer_adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999,
                             epsilon = NULL, decay = 0.0000000000000, amsgrad = FALSE, clipnorm = NULL,
                             clipvalue = NULL),
  metrics = c('accuracy')
)

# Train model
history = model %>% fit(
  Input, YTrain,
  batch_size = 512,#512
  epochs = 100,
  validation_split = 0.02
)

FullOut = c()
Counter = 1

Mem = 0

DataArray = PredictArray[,]
DataArray = DataArray[which(DataArray[,37] %in% ClassifyGroup),]
class(DataArray) = 'numeric'

XPredict = as.array(DataArray[,c(4:29,31)])
XPredict[which(is.infinite(XPredict))] = -4        #Assume all infs are neg infs in spi or spei caused by 0 precip months
XPredict[which(is.na(XPredict))] = -4        #Assume all infs are neg infs in spi or spei caused by 0 precip months
YPredict = as.array(DataArray[,30])
class(YPredict) = 'integer'

if(length(YPredict) == 0) next

#XPredict[which(XPredict[,27] == 0),27] = 1
#XPredict[,27] = XPredict[,27] - 1
XPredictSub = XPredict[,c(27,1,2:13,14:25,26)]

XPredictSub[which(XPredictSub[,1] == 0),1] = 1
XPredictSub[,1] = XPredictSub[,1] - 1

XPredictSubNorm = XPredictSub
for(i in 1:dim(XPredictSubNorm)[2]){
  
  XPredictSubNorm[,i] = Normalize(XPredictSubNorm[,i], NormMaxes[i], NormMins[i])
  
}

Loaded = predict(pca, XPredictSubNorm)[,1:8]

YPredict[which(YPredict == 0)] = 1
YPredict = YPredict - 1

Results = predict(model, Loaded)
ResultsClass = apply(Results, 1, FUN = which.max) - 1

#table(ResultsClass, YPredict)
table(ResultsClass, XPredictSub[,8])

Output = cbind(DataArray[,1], DataArray[,2])
Output = cbind(Output, ResultsClass)

#View(XPredict[which(ResultsClass == 1),])

if(!dir.exists(paste0(ClassificationOutputDir, HindcastDate, '\\'))) dir.create(paste0(ClassificationOutputDir, HindcastDate, '_Hind\\'))
write.csv(Output, paste0(ClassificationOutputDir, HindcastDate, '\\', Mem, '.csv'), row.names=FALSE, quote=FALSE)

Output = cbind(DataArray[,1], DataArray[,2])
Output = cbind(Output, XPredictSub[,8])

if(!dir.exists(paste0(PreviousOutputDir, HindcastDate, '\\'))) dir.create(paste0(PreviousOutputDir, HindcastDate, '_Hind\\'))
write.csv(Output, paste0(PreviousOutputDir, HindcastDate, '\\', Mem, '.csv'), row.names=FALSE, quote=FALSE)

if(Mem == 1) FullOut = cbind(FullOut, DataArray[,1], DataArray[,2])
FullOut = cbind(FullOut, ResultsClass)
  
